{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T21:24:50.024915Z",
          "iopub.status.busy": "2024-02-07T21:24:50.023647Z",
          "iopub.status.idle": "2024-02-07T21:24:54.729412Z",
          "shell.execute_reply": "2024-02-07T21:24:54.728256Z",
          "shell.execute_reply.started": "2024-02-07T21:24:50.024838Z"
        },
        "id": "TqANzYFVnnY2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "dataPath = \"C:/Users/Maevex/Desktop/Lujain/home-credit-credit-risk-model-stability/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reading the Train and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Maevex\\AppData\\Local\\Temp\\ipykernel_20684\\937885079.py:4: DtypeWarning: Columns (20,45,46,53,57,84,143,146,167) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train_static_0_0 = pd.read_csv(dataPath + \"csv_files/train/train_static_0_0.csv\")\n",
            "C:\\Users\\Maevex\\AppData\\Local\\Temp\\ipykernel_20684\\937885079.py:6: DtypeWarning: Columns (20,45,46,56,57,84,143,146,167) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train_static_0_1 = pd.read_csv(dataPath + \"csv_files/train/train_static_0_1.csv\")\n",
            "C:\\Users\\Maevex\\AppData\\Local\\Temp\\ipykernel_20684\\937885079.py:11: DtypeWarning: Columns (1,2,3,4,7,45,46,47,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train_static_cb = pd.read_csv(dataPath + \"csv_files/train/train_static_cb_0.csv\")\n",
            "C:\\Users\\Maevex\\AppData\\Local\\Temp\\ipykernel_20684\\937885079.py:13: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train_person_1 = pd.read_csv(dataPath + \"csv_files/train/train_person_1.csv\")\n"
          ]
        }
      ],
      "source": [
        "# Read CSV files\n",
        "train_basetable = pd.read_csv(dataPath + \"csv_files/train/train_base.csv\")\n",
        "\n",
        "train_static_0_0 = pd.read_csv(dataPath + \"csv_files/train/train_static_0_0.csv\")\n",
        "\n",
        "train_static_0_1 = pd.read_csv(dataPath + \"csv_files/train/train_static_0_1.csv\")\n",
        "\n",
        "# Concatenate the DataFrames vertically\n",
        "train_static = pd.concat([train_static_0_0, train_static_0_1], axis=0, ignore_index=True)\n",
        "\n",
        "train_static_cb = pd.read_csv(dataPath + \"csv_files/train/train_static_cb_0.csv\")\n",
        "\n",
        "train_person_1 = pd.read_csv(dataPath + \"csv_files/train/train_person_1.csv\")\n",
        "\n",
        "train_credit_bureau_b_2 = pd.read_csv(dataPath + \"csv_files/train/train_credit_bureau_b_2.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read CSV files for test data\n",
        "test_basetable = pd.read_csv(dataPath + \"csv_files/test/test_base.csv\")\n",
        "\n",
        "test_static_0_0 = pd.read_csv(dataPath + \"csv_files/test/test_static_0_0.csv\")\n",
        "\n",
        "test_static_0_1 = pd.read_csv(dataPath + \"csv_files/test/test_static_0_1.csv\")\n",
        "\n",
        "test_static_0_2 = pd.read_csv(dataPath + \"csv_files/test/test_static_0_2.csv\")\n",
        "\n",
        "# Concatenate the DataFrames vertically\n",
        "test_static = pd.concat([test_static_0_0, test_static_0_1, test_static_0_2], axis=0, ignore_index=True)\n",
        "\n",
        "test_static_cb = pd.read_csv(dataPath + \"csv_files/test/test_static_cb_0.csv\")\n",
        "\n",
        "test_person_1 = pd.read_csv(dataPath + \"csv_files/test/test_person_1.csv\")\n",
        "\n",
        "test_credit_bureau_b_2 = pd.read_csv(dataPath + \"csv_files/test/test_credit_bureau_b_2.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finding nulls values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to find null values and their percentages\n",
        "def find_nulls(df):\n",
        "    null_counts = df.isnull().sum()\n",
        "    total_counts = len(df)\n",
        "    null_percentages = (null_counts / total_counts) * 100\n",
        "    return pd.DataFrame({\n",
        "        'Null Count': null_counts,\n",
        "        'Null Percentage': null_percentages\n",
        "    })\n",
        "\n",
        "\n",
        "# Finding nulls in each dataframe for the train data\n",
        "train_nulls = {\n",
        "    \"train_basetable\": find_nulls(train_basetable),\n",
        "    \"train_static\": find_nulls(train_static),\n",
        "    \"train_static_cb\": find_nulls(train_static_cb),\n",
        "    \"train_person_1\": find_nulls(train_person_1),\n",
        "    \"train_credit_bureau_b_2\": find_nulls(train_credit_bureau_b_2)\n",
        "}\n",
        "\n",
        "# Convert the dictionaries to DataFrames\n",
        "train_nulls_df = pd.concat(train_nulls, axis=1)\n",
        "\n",
        "# Exporting the nulls count to an Excel file\n",
        "with pd.ExcelWriter(dataPath + 'null_counts.xlsx') as writer:\n",
        "    train_nulls_df.to_excel(writer, sheet_name='train_nulls', index=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Casting types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T21:24:54.733356Z",
          "iopub.status.busy": "2024-02-07T21:24:54.732337Z",
          "iopub.status.idle": "2024-02-07T21:24:54.746195Z",
          "shell.execute_reply": "2024-02-07T21:24:54.744741Z",
          "shell.execute_reply.started": "2024-02-07T21:24:54.733288Z"
        },
        "id": "W6O6iJtnnnY2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def determine_dtype(col_name):\n",
        "    if col_name[-1] == 'D':\n",
        "        return 'datetime64[ns]'\n",
        "    elif col_name[-1] == 'L':\n",
        "        return 'object'\n",
        "    elif col_name[-1] == 'A':\n",
        "        return 'float64'\n",
        "    elif col_name[-1] == 'M':\n",
        "        return 'object'\n",
        "    elif col_name[-1] == 'P':\n",
        "        return 'float64'\n",
        "    elif col_name[-1] == 'T':\n",
        "        return 'object'\n",
        "    else:\n",
        "        return 'object'\n",
        "\n",
        "def set_table_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    for col in df.columns:\n",
        "        dtype = determine_dtype(col)\n",
        "        df[col] = df[col].astype(dtype)\n",
        "    return df\n",
        "\n",
        "def convert_strings(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype.name in ['object', 'string']:\n",
        "            df[col] = df[col].astype(\"string\").astype('category')\n",
        "            current_categories = df[col].cat.categories\n",
        "            new_categories = current_categories.to_list() + [\"Unknown\"]\n",
        "            new_dtype = pd.CategoricalDtype(categories=new_categories, ordered=True)\n",
        "            df[col] = df[col].astype(new_dtype)\n",
        "    return df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read CSV files\n",
        "train_static_0_0 = set_table_dtypes(train_static_0_0)\n",
        "train_static_0_1 = set_table_dtypes(train_static_0_1)\n",
        "train_static_cb = set_table_dtypes(train_static_cb)\n",
        "train_person_1 = set_table_dtypes(train_person_1)\n",
        "train_credit_bureau_b_2 = set_table_dtypes(train_credit_bureau_b_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read CSV files for test data\n",
        "test_static_0_0 = set_table_dtypes(test_static_0_0)\n",
        "test_static_0_1 = set_table_dtypes(test_static_0_1)\n",
        "test_static_0_2 = set_table_dtypes(test_static_0_2) # NOT NEEDED\n",
        "test_static_cb = set_table_dtypes(test_static_cb)\n",
        "test_person_1 = set_table_dtypes(test_person_1)\n",
        "test_credit_bureau_b_2 = set_table_dtypes(test_credit_bureau_b_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Merging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T21:25:16.24119Z",
          "iopub.status.busy": "2024-02-07T21:25:16.24057Z",
          "iopub.status.idle": "2024-02-07T21:25:18.454532Z",
          "shell.execute_reply": "2024-02-07T21:25:18.452798Z",
          "shell.execute_reply.started": "2024-02-07T21:25:16.241152Z"
        },
        "id": "X9RSgodVnnY3",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['amtinstpaidbefduel24m_4187115A', 'annuity_780A', 'annuitynextmonth_57A', 'avginstallast24m_3658937A', 'avglnamtstart24m_4525187A', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'credamount_770A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'disbursedcredamount_1113A', 'downpmt_116A', 'inittransactionamount_650A', 'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M', 'lastapprcredamount_781A', 'lastcancelreason_561M', 'lastotherinc_902A', 'lastotherlnsexpense_631A', 'lastrejectcommoditycat_161M', 'lastrejectcommodtypec_5251769M', 'lastrejectcredamount_222A', 'lastrejectreason_759M', 'lastrejectreasonclient_4145040M', 'maininc_215A', 'maxannuity_159A', 'maxannuity_4075009A', 'maxdebt4_972A', 'maxinstallast24m_3658928A', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'maxpmtlast3m_4525190A', 'previouscontdistrict_112M', 'price_1097A', 'sumoutstandtotal_3546847A', 'sumoutstandtotalest_4493215A', 'totaldebt_9A', 'totalsettled_863A', 'totinstallast1m_4525188A']\n",
            "['description_5085714M', 'education_1103M', 'education_88M', 'maritalst_385M', 'maritalst_893M', 'pmtaverage_3A', 'pmtaverage_4527227A', 'pmtaverage_4955615A', 'pmtssum_45A']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the aggregation function\n",
        "def aggregate_train_person_1_feats_1(df):\n",
        "    agg_df = df.groupby(\"case_id\").agg(\n",
        "        mainoccupationinc_384A_max=pd.NamedAgg(column=\"mainoccupationinc_384A\", aggfunc=\"max\"),\n",
        "        mainoccupationinc_384A_any_selfemployed=pd.NamedAgg(column=\"incometype_1044T\", aggfunc=lambda x: (x == \"SELFEMPLOYED\").max())\n",
        "    ).reset_index()\n",
        "    return agg_df\n",
        "\n",
        "# Apply the aggregation function\n",
        "train_person_1_feats_1 = aggregate_train_person_1_feats_1(train_person_1)\n",
        "\n",
        "# Filter and select operations\n",
        "train_person_1_feats_2 = train_person_1.loc[train_person_1[\"num_group1\"] == 0, [\"case_id\", \"housetype_905L\"]]\n",
        "train_person_1_feats_2.rename(columns={\"housetype_905L\": \"person_housetype\"}, inplace=True)\n",
        "\n",
        "# Define the aggregation function for another table\n",
        "def aggregate_train_credit_bureau_b_2_feats(df):\n",
        "    agg_df = df.groupby(\"case_id\").agg(\n",
        "        pmts_pmtsoverdue_635A_max=pd.NamedAgg(column=\"pmts_pmtsoverdue_635A\", aggfunc=\"max\"),\n",
        "        pmts_dpdvalue_108P_over31=pd.NamedAgg(column=\"pmts_dpdvalue_108P\", aggfunc=lambda x: (x > 31).max())\n",
        "    ).reset_index()\n",
        "    return agg_df\n",
        "\n",
        "# Apply the aggregation function\n",
        "train_credit_bureau_b_2_feats = aggregate_train_credit_bureau_b_2_feats(train_credit_bureau_b_2)\n",
        "\n",
        "# Selecting columns based on their suffix\n",
        "selected_static_cols = [col for col in train_static.columns if col[-1] in (\"A\", \"M\")]\n",
        "print(selected_static_cols)\n",
        "\n",
        "selected_static_cb_cols = [col for col in train_static_cb.columns if col[-1] in (\"A\", \"M\")]\n",
        "print(selected_static_cb_cols)\n",
        "\n",
        "# Joining tables together\n",
        "data = train_basetable.merge(\n",
        "    train_static[[\"case_id\"] + selected_static_cols], how=\"left\", on=\"case_id\"\n",
        ").merge(\n",
        "    train_static_cb[[\"case_id\"] + selected_static_cb_cols], how=\"left\", on=\"case_id\"\n",
        ").merge(\n",
        "    train_person_1_feats_1, how=\"left\", on=\"case_id\"\n",
        ").merge(\n",
        "    train_person_1_feats_2, how=\"left\", on=\"case_id\"\n",
        ").merge(\n",
        "    train_credit_bureau_b_2_feats, how=\"left\", on=\"case_id\"\n",
        ")\n",
        "\n",
        "\n",
        "#1m 51s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T21:25:18.456308Z",
          "iopub.status.busy": "2024-02-07T21:25:18.455848Z",
          "iopub.status.idle": "2024-02-07T21:25:18.474933Z",
          "shell.execute_reply": "2024-02-07T21:25:18.473491Z",
          "shell.execute_reply.started": "2024-02-07T21:25:18.456258Z"
        },
        "id": "j8jeyWoqnnY3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the aggregation function for test_person_1_feats_1\n",
        "def aggregate_test_person_1_feats_1(df):\n",
        "    agg_df = df.groupby(\"case_id\").agg(\n",
        "        mainoccupationinc_384A_max=pd.NamedAgg(column=\"mainoccupationinc_384A\", aggfunc=\"max\"),\n",
        "        mainoccupationinc_384A_any_selfemployed=pd.NamedAgg(column=\"incometype_1044T\", aggfunc=lambda x: (x == \"SELFEMPLOYED\").max())\n",
        "    ).reset_index()\n",
        "    return agg_df\n",
        "\n",
        "# Apply the aggregation function\n",
        "test_person_1_feats_1 = aggregate_test_person_1_feats_1(test_person_1)\n",
        "\n",
        "# Filter and select operations for test_person_1_feats_2\n",
        "test_person_1_feats_2 = test_person_1.loc[test_person_1[\"num_group1\"] == 0, [\"case_id\", \"housetype_905L\"]]\n",
        "test_person_1_feats_2.rename(columns={\"housetype_905L\": \"person_housetype\"}, inplace=True)\n",
        "\n",
        "# Define the aggregation function for test_credit_bureau_b_2_feats\n",
        "def aggregate_test_credit_bureau_b_2_feats(df):\n",
        "    agg_df = df.groupby(\"case_id\").agg(\n",
        "        pmts_pmtsoverdue_635A_max=pd.NamedAgg(column=\"pmts_pmtsoverdue_635A\", aggfunc=\"max\"),\n",
        "        pmts_dpdvalue_108P_over31=pd.NamedAgg(column=\"pmts_dpdvalue_108P\", aggfunc=lambda x: (x > 31).max())\n",
        "    ).reset_index()\n",
        "    return agg_df\n",
        "\n",
        "# Apply the aggregation function\n",
        "test_credit_bureau_b_2_feats = aggregate_test_credit_bureau_b_2_feats(test_credit_bureau_b_2)\n",
        "\n",
        "# Joining tables together\n",
        "data_submission = test_basetable.merge(\n",
        "    test_static[[\"case_id\"] + selected_static_cols], how=\"left\", on=\"case_id\"\n",
        ").merge(\n",
        "    test_static_cb[[\"case_id\"] + selected_static_cb_cols], how=\"left\", on=\"case_id\"\n",
        ").merge(\n",
        "    test_person_1_feats_1, how=\"left\", on=\"case_id\"\n",
        ").merge(\n",
        "    test_person_1_feats_2, how=\"left\", on=\"case_id\"\n",
        ").merge(\n",
        "    test_credit_bureau_b_2_feats, how=\"left\", on=\"case_id\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T21:25:18.477539Z",
          "iopub.status.busy": "2024-02-07T21:25:18.47705Z",
          "iopub.status.idle": "2024-02-07T21:25:28.525841Z",
          "shell.execute_reply": "2024-02-07T21:25:28.524407Z",
          "shell.execute_reply.started": "2024-02-07T21:25:18.477494Z"
        },
        "id": "n1A0qUWXnnY4",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['amtinstpaidbefduel24m_4187115A', 'annuity_780A', 'annuitynextmonth_57A', 'avginstallast24m_3658937A', 'avglnamtstart24m_4525187A', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'credamount_770A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'disbursedcredamount_1113A', 'downpmt_116A', 'inittransactionamount_650A', 'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M', 'lastapprcredamount_781A', 'lastcancelreason_561M', 'lastotherinc_902A', 'lastotherlnsexpense_631A', 'lastrejectcommoditycat_161M', 'lastrejectcommodtypec_5251769M', 'lastrejectcredamount_222A', 'lastrejectreason_759M', 'lastrejectreasonclient_4145040M', 'maininc_215A', 'maxannuity_159A', 'maxannuity_4075009A', 'maxdebt4_972A', 'maxinstallast24m_3658928A', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'maxpmtlast3m_4525190A', 'previouscontdistrict_112M', 'price_1097A', 'sumoutstandtotal_3546847A', 'sumoutstandtotalest_4493215A', 'totaldebt_9A', 'totalsettled_863A', 'totinstallast1m_4525188A', 'description_5085714M', 'education_1103M', 'education_88M', 'maritalst_385M', 'maritalst_893M', 'pmtaverage_3A', 'pmtaverage_4527227A', 'pmtaverage_4955615A', 'pmtssum_45A']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Shuffle the case_ids and split into train, validation, and test sets\n",
        "case_ids = data[\"case_id\"].unique()\n",
        "case_ids = pd.Series(case_ids).sample(frac=1, random_state=1).to_numpy()\n",
        "case_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.6, random_state=1)\n",
        "case_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=1)\n",
        "\n",
        "# Extract columns for prediction\n",
        "cols_pred = [col for col in data.columns if col[-1].isupper() and col[:-1].islower()]\n",
        "\n",
        "print(cols_pred)\n",
        "\n",
        "# Function to filter data and convert to pandas DataFrame\n",
        "def from_polars_to_pandas(case_ids, data):\n",
        "    filtered_data = data[data[\"case_id\"].isin(case_ids)]\n",
        "    return (\n",
        "        filtered_data[[\"case_id\", \"WEEK_NUM\", \"target\"]],\n",
        "        filtered_data[cols_pred],\n",
        "        filtered_data[\"target\"]\n",
        "    )\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "base_train, X_train, y_train = from_polars_to_pandas(case_ids_train, data)\n",
        "base_valid, X_valid, y_valid = from_polars_to_pandas(case_ids_valid, data)\n",
        "base_test, X_test, y_test = from_polars_to_pandas(case_ids_test, data)\n",
        "\n",
        "# Convert string columns to category\n",
        "for df in [X_train, X_valid, X_test]:\n",
        "    df = convert_strings(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T21:25:28.529014Z",
          "iopub.status.busy": "2024-02-07T21:25:28.52813Z",
          "iopub.status.idle": "2024-02-07T21:25:28.534479Z",
          "shell.execute_reply": "2024-02-07T21:25:28.53334Z",
          "shell.execute_reply.started": "2024-02-07T21:25:28.528913Z"
        },
        "id": "HnYkfG5xnnY4",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: (915995, 48)\n",
            "Valid: (305332, 48)\n",
            "Test: (305332, 48)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train: {X_train.shape}\")\n",
        "print(f\"Valid: {X_valid.shape}\")\n",
        "print(f\"Test: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lightgmb model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Maevex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 10 rounds\n",
            "[50]\tvalid_0's auc: 0.699919\n",
            "[100]\tvalid_0's auc: 0.717512\n",
            "[150]\tvalid_0's auc: 0.7247\n",
            "[200]\tvalid_0's auc: 0.728744\n",
            "[250]\tvalid_0's auc: 0.731757\n",
            "[300]\tvalid_0's auc: 0.734638\n",
            "[350]\tvalid_0's auc: 0.736952\n",
            "[400]\tvalid_0's auc: 0.738423\n",
            "[450]\tvalid_0's auc: 0.739968\n",
            "[500]\tvalid_0's auc: 0.74133\n",
            "[550]\tvalid_0's auc: 0.742478\n",
            "[600]\tvalid_0's auc: 0.743322\n",
            "[650]\tvalid_0's auc: 0.744115\n",
            "[700]\tvalid_0's auc: 0.744701\n",
            "[750]\tvalid_0's auc: 0.745422\n",
            "Early stopping, best iteration is:\n",
            "[765]\tvalid_0's auc: 0.745759\n"
          ]
        }
      ],
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
        "lgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n",
        "\n",
        "params = {\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"objective\": \"binary\",\n",
        "    \"metric\": \"auc\",\n",
        "    \"max_depth\": 3,\n",
        "    \"num_leaves\": 31,\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"feature_fraction\": 0.9,\n",
        "    \"bagging_fraction\": 0.8,\n",
        "    \"bagging_freq\": 5,\n",
        "    \"n_estimators\": 1000,\n",
        "    \"verbose\": -1,\n",
        "}\n",
        "\n",
        "gbm = lgb.train(\n",
        "    params,\n",
        "    lgb_train,\n",
        "    valid_sets=lgb_valid,\n",
        "    callbacks=[lgb.log_evaluation(50), lgb.early_stopping(10)]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nueral Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'X_train' and 'X_valid' are pandas DataFrames\n",
        "X_train = pd.get_dummies(X_train, drop_first=True)\n",
        "X_valid = pd.get_dummies(X_valid, drop_first=True)\n",
        "\n",
        "# Ensure alignment of features in training and validation set\n",
        "X_train, X_valid = X_train.align(X_valid, join='inner', axis=1)  # align columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Label encoding for each categorical column\n",
        "for column in X_train.select_dtypes(include=['object', 'category']).columns:\n",
        "    le = LabelEncoder()\n",
        "    X_train[column] = le.fit_transform(X_train[column].astype(str))\n",
        "    X_valid[column] = le.transform(X_valid[column].astype(str))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_valid = X_valid.astype('float32')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Maevex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "\u001b[1m28625/28625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1ms/step - AUC: 0.5038 - loss: 0.1432 - val_AUC: 0.5000 - val_loss: 0.1395\n",
            "Epoch 2/1000\n",
            "\u001b[1m  121/28625\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 834us/step - AUC: 0.5030 - loss: 0.1551   "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Maevex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\callbacks\\early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: AUC,loss,val_AUC,val_loss\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m28625/28625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 985us/step - AUC: 0.4991 - loss: 0.1406 - val_AUC: 0.5000 - val_loss: 0.1394\n",
            "Epoch 3/1000\n",
            "\u001b[1m28625/28625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1ms/step - AUC: 0.4992 - loss: 0.1401 - val_AUC: 0.5000 - val_loss: 0.1391\n",
            "Epoch 4/1000\n",
            "\u001b[1m28625/28625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 997us/step - AUC: 0.4998 - loss: 0.1402 - val_AUC: 0.5000 - val_loss: 0.1401\n",
            "Epoch 5/1000\n",
            "\u001b[1m28625/28625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 998us/step - AUC: 0.4988 - loss: 0.1400 - val_AUC: 0.5000 - val_loss: 0.1396\n",
            "Epoch 6/1000\n",
            "\u001b[1m28625/28625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1000us/step - AUC: 0.4976 - loss: 0.1402 - val_AUC: 0.5000 - val_loss: 0.1391\n",
            "Epoch 7/1000\n",
            "\u001b[1m28625/28625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 993us/step - AUC: 0.4997 - loss: 0.1394 - val_AUC: 0.5000 - val_loss: 0.1391\n",
            "Epoch 8/1000\n",
            "\u001b[1m28625/28625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 985us/step - AUC: 0.5013 - loss: 0.1391 - val_AUC: 0.5000 - val_loss: 0.1400\n",
            "Epoch 9/1000\n",
            "\u001b[1m28017/28625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 824us/step - AUC: 0.4977 - loss: 0.1398"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define the model architecture\n",
        "model = models.Sequential([\n",
        "    layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer\n",
        "    layers.Dropout(0.2),  # Dropout for regularization\n",
        "    layers.Dense(64, activation='relu'),  # Hidden layer\n",
        "    layers.Dropout(0.2),  # Dropout for regularization\n",
        "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model with appropriate loss function, optimizer and metrics\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.05),\n",
        "    loss='binary_crossentropy',  # Appropriate loss for binary classification\n",
        "    metrics=['AUC']  # Metric as AUC, consistent with your LightGBM model\n",
        ")\n",
        "\n",
        "# Define Early Stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_auc', patience=10, mode='max', restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    epochs=1000,  # Maximum number of epochs\n",
        "    batch_size=32,  # Batch size for the training\n",
        "    callbacks=[early_stopping],  # Callbacks for early stopping\n",
        "    verbose=1\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Home Credit 2024 Starter Notebook",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 7493015,
          "sourceId": 50160,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30635,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
