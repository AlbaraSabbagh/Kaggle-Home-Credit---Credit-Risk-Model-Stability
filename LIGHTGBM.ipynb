{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T21:24:50.024915Z",
          "iopub.status.busy": "2024-02-07T21:24:50.023647Z",
          "iopub.status.idle": "2024-02-07T21:24:54.729412Z",
          "shell.execute_reply": "2024-02-07T21:24:54.728256Z",
          "shell.execute_reply.started": "2024-02-07T21:24:50.024838Z"
        },
        "id": "TqANzYFVnnY2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "dataPath = \"C:/Users/Maevex/Desktop/Lujain/home-credit-credit-risk-model-stability/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reading the Train and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Maevex\\AppData\\Local\\Temp\\ipykernel_9768\\937885079.py:4: DtypeWarning: Columns (20,45,46,53,57,84,143,146,167) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train_static_0_0 = pd.read_csv(dataPath + \"csv_files/train/train_static_0_0.csv\")\n",
            "C:\\Users\\Maevex\\AppData\\Local\\Temp\\ipykernel_9768\\937885079.py:6: DtypeWarning: Columns (20,45,46,56,57,84,143,146,167) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train_static_0_1 = pd.read_csv(dataPath + \"csv_files/train/train_static_0_1.csv\")\n",
            "C:\\Users\\Maevex\\AppData\\Local\\Temp\\ipykernel_9768\\937885079.py:11: DtypeWarning: Columns (1,2,3,4,7,45,46,47,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train_static_cb = pd.read_csv(dataPath + \"csv_files/train/train_static_cb_0.csv\")\n",
            "C:\\Users\\Maevex\\AppData\\Local\\Temp\\ipykernel_9768\\937885079.py:13: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train_person_1 = pd.read_csv(dataPath + \"csv_files/train/train_person_1.csv\")\n"
          ]
        }
      ],
      "source": [
        "# Read CSV files\n",
        "train_basetable = pd.read_csv(dataPath + \"csv_files/train/train_base.csv\")\n",
        "\n",
        "train_static_0_0 = pd.read_csv(dataPath + \"csv_files/train/train_static_0_0.csv\")\n",
        "\n",
        "train_static_0_1 = pd.read_csv(dataPath + \"csv_files/train/train_static_0_1.csv\")\n",
        "\n",
        "# Concatenate the DataFrames vertically\n",
        "train_static = pd.concat([train_static_0_0, train_static_0_1], axis=0, ignore_index=True)\n",
        "\n",
        "train_static_cb = pd.read_csv(dataPath + \"csv_files/train/train_static_cb_0.csv\")\n",
        "\n",
        "train_person_1 = pd.read_csv(dataPath + \"csv_files/train/train_person_1.csv\")\n",
        "\n",
        "train_credit_bureau_b_2 = pd.read_csv(dataPath + \"csv_files/train/train_credit_bureau_b_2.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read CSV files for test data\n",
        "test_basetable = pd.read_csv(dataPath + \"csv_files/test/test_base.csv\")\n",
        "\n",
        "test_static_0_0 = pd.read_csv(dataPath + \"csv_files/test/test_static_0_0.csv\")\n",
        "\n",
        "test_static_0_1 = pd.read_csv(dataPath + \"csv_files/test/test_static_0_1.csv\")\n",
        "\n",
        "test_static_0_2 = pd.read_csv(dataPath + \"csv_files/test/test_static_0_2.csv\")\n",
        "\n",
        "# Concatenate the DataFrames vertically\n",
        "test_static = pd.concat([test_static_0_0, test_static_0_1, test_static_0_2], axis=0, ignore_index=True)\n",
        "\n",
        "test_static_cb = pd.read_csv(dataPath + \"csv_files/test/test_static_cb_0.csv\")\n",
        "\n",
        "test_person_1 = pd.read_csv(dataPath + \"csv_files/test/test_person_1.csv\")\n",
        "\n",
        "test_credit_bureau_b_2 = pd.read_csv(dataPath + \"csv_files/test/test_credit_bureau_b_2.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finding nulls values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to find null values and their percentages\n",
        "def find_nulls(df):\n",
        "    null_counts = df.isnull().sum()\n",
        "    total_counts = len(df)\n",
        "    null_percentages = (null_counts / total_counts) * 100\n",
        "    return pd.DataFrame({\n",
        "        'Null Count': null_counts,\n",
        "        'Null Percentage': null_percentages\n",
        "    })\n",
        "\n",
        "\n",
        "# Finding nulls in each dataframe for the train data\n",
        "train_nulls = {\n",
        "    \"train_basetable\": find_nulls(train_basetable),\n",
        "    \"train_static\": find_nulls(train_static),\n",
        "    \"train_static_cb\": find_nulls(train_static_cb),\n",
        "    \"train_person_1\": find_nulls(train_person_1),\n",
        "    \"train_credit_bureau_b_2\": find_nulls(train_credit_bureau_b_2)\n",
        "}\n",
        "\n",
        "# Convert the dictionaries to DataFrames\n",
        "train_nulls_df = pd.concat(train_nulls, axis=1)\n",
        "\n",
        "# Exporting the nulls count to an Excel file\n",
        "# with pd.ExcelWriter(dataPath + 'null_counts.xlsx') as writer:\n",
        "#     train_nulls_df.to_excel(writer, sheet_name='train_nulls', index=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Casting types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T21:24:54.733356Z",
          "iopub.status.busy": "2024-02-07T21:24:54.732337Z",
          "iopub.status.idle": "2024-02-07T21:24:54.746195Z",
          "shell.execute_reply": "2024-02-07T21:24:54.744741Z",
          "shell.execute_reply.started": "2024-02-07T21:24:54.733288Z"
        },
        "id": "W6O6iJtnnnY2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def determine_dtype(col_name):\n",
        "    if col_name[-1] == 'D':\n",
        "        return 'datetime64[ns]'\n",
        "    elif col_name[-1] == 'L':\n",
        "        return 'object'\n",
        "    elif col_name[-1] == 'A':\n",
        "        return 'float64'\n",
        "    elif col_name[-1] == 'M':\n",
        "        return 'object'\n",
        "    elif col_name[-1] == 'P':\n",
        "        return 'float64'\n",
        "    elif col_name[-1] == 'T':\n",
        "        return 'object'\n",
        "    else:\n",
        "        return 'object'\n",
        "\n",
        "def set_table_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    for col in df.columns:\n",
        "        dtype = determine_dtype(col)\n",
        "        df[col] = df[col].astype(dtype)\n",
        "    return df\n",
        "\n",
        "def convert_strings(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype.name in ['object', 'string']:\n",
        "            df[col] = df[col].astype(\"string\").astype('category')\n",
        "            current_categories = df[col].cat.categories\n",
        "            new_categories = current_categories.to_list() + [\"Unknown\"]\n",
        "            new_dtype = pd.CategoricalDtype(categories=new_categories, ordered=True)\n",
        "            df[col] = df[col].astype(new_dtype)\n",
        "    return df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read CSV files\n",
        "train_static_0_0 = set_table_dtypes(train_static_0_0)\n",
        "train_static_0_1 = set_table_dtypes(train_static_0_1)\n",
        "train_static_cb = set_table_dtypes(train_static_cb)\n",
        "train_person_1 = set_table_dtypes(train_person_1)\n",
        "train_credit_bureau_b_2 = set_table_dtypes(train_credit_bureau_b_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read CSV files for test data\n",
        "test_static_0_0 = set_table_dtypes(test_static_0_0)\n",
        "test_static_0_1 = set_table_dtypes(test_static_0_1)\n",
        "test_static_0_2 = set_table_dtypes(test_static_0_2) # NOT NEEDED\n",
        "test_static_cb = set_table_dtypes(test_static_cb)\n",
        "test_person_1 = set_table_dtypes(test_person_1)\n",
        "test_credit_bureau_b_2 = set_table_dtypes(test_credit_bureau_b_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Merging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T21:25:16.24119Z",
          "iopub.status.busy": "2024-02-07T21:25:16.24057Z",
          "iopub.status.idle": "2024-02-07T21:25:18.454532Z",
          "shell.execute_reply": "2024-02-07T21:25:18.452798Z",
          "shell.execute_reply.started": "2024-02-07T21:25:16.241152Z"
        },
        "id": "X9RSgodVnnY3",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['amtinstpaidbefduel24m_4187115A', 'annuity_780A', 'annuitynextmonth_57A', 'avginstallast24m_3658937A', 'avglnamtstart24m_4525187A', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'credamount_770A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'disbursedcredamount_1113A', 'downpmt_116A', 'inittransactionamount_650A', 'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M', 'lastapprcredamount_781A', 'lastcancelreason_561M', 'lastotherinc_902A', 'lastotherlnsexpense_631A', 'lastrejectcommoditycat_161M', 'lastrejectcommodtypec_5251769M', 'lastrejectcredamount_222A', 'lastrejectreason_759M', 'lastrejectreasonclient_4145040M', 'maininc_215A', 'maxannuity_159A', 'maxannuity_4075009A', 'maxdebt4_972A', 'maxinstallast24m_3658928A', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'maxpmtlast3m_4525190A', 'previouscontdistrict_112M', 'price_1097A', 'sumoutstandtotal_3546847A', 'sumoutstandtotalest_4493215A', 'totaldebt_9A', 'totalsettled_863A', 'totinstallast1m_4525188A']\n",
            "['description_5085714M', 'education_1103M', 'education_88M', 'maritalst_385M', 'maritalst_893M', 'pmtaverage_3A', 'pmtaverage_4527227A', 'pmtaverage_4955615A', 'pmtssum_45A']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the aggregation function\n",
        "def aggregate_train_person_1_feats_1(df):\n",
        "    agg_df = df.groupby(\"case_id\").agg(\n",
        "        mainoccupationinc_384A_max=pd.NamedAgg(column=\"mainoccupationinc_384A\", aggfunc=\"max\"),\n",
        "        mainoccupationinc_384A_any_selfemployed=pd.NamedAgg(column=\"incometype_1044T\", aggfunc=lambda x: (x == \"SELFEMPLOYED\").max())\n",
        "    ).reset_index()\n",
        "    return agg_df\n",
        "\n",
        "# Apply the aggregation function\n",
        "train_person_1_feats_1 = aggregate_train_person_1_feats_1(train_person_1)\n",
        "\n",
        "# Filter and select operations\n",
        "train_person_1_feats_2 = train_person_1.loc[train_person_1[\"num_group1\"] == 0, [\"case_id\", \"housetype_905L\"]]\n",
        "train_person_1_feats_2.rename(columns={\"housetype_905L\": \"person_housetype\"}, inplace=True)\n",
        "\n",
        "# Define the aggregation function for another table\n",
        "def aggregate_train_credit_bureau_b_2_feats(df):\n",
        "    agg_df = df.groupby(\"case_id\").agg(\n",
        "        pmts_pmtsoverdue_635A_max=pd.NamedAgg(column=\"pmts_pmtsoverdue_635A\", aggfunc=\"max\"),\n",
        "        pmts_dpdvalue_108P_over31=pd.NamedAgg(column=\"pmts_dpdvalue_108P\", aggfunc=lambda x: (x > 31).max())\n",
        "    ).reset_index()\n",
        "    return agg_df\n",
        "\n",
        "# Apply the aggregation function\n",
        "train_credit_bureau_b_2_feats = aggregate_train_credit_bureau_b_2_feats(train_credit_bureau_b_2)\n",
        "\n",
        "# Selecting columns based on their suffix\n",
        "selected_static_cols = [col for col in train_static.columns if col[-1] in (\"A\", \"M\")]\n",
        "print(selected_static_cols)\n",
        "\n",
        "selected_static_cb_cols = [col for col in train_static_cb.columns if col[-1] in (\"A\", \"M\")]\n",
        "print(selected_static_cb_cols)\n",
        "\n",
        "# Joining tables together\n",
        "data = train_basetable.merge(\n",
        "    train_static[[\"case_id\"] + selected_static_cols], how=\"left\", on=\"case_id\"\n",
        ").merge(\n",
        "    train_static_cb[[\"case_id\"] + selected_static_cb_cols], how=\"left\", on=\"case_id\"\n",
        ").merge(\n",
        "    train_person_1_feats_1, how=\"left\", on=\"case_id\"\n",
        ").merge(\n",
        "    train_person_1_feats_2, how=\"left\", on=\"case_id\"\n",
        ").merge(\n",
        "    train_credit_bureau_b_2_feats, how=\"left\", on=\"case_id\"\n",
        ")\n",
        "\n",
        "\n",
        "#1m 51s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T21:25:18.456308Z",
          "iopub.status.busy": "2024-02-07T21:25:18.455848Z",
          "iopub.status.idle": "2024-02-07T21:25:18.474933Z",
          "shell.execute_reply": "2024-02-07T21:25:18.473491Z",
          "shell.execute_reply.started": "2024-02-07T21:25:18.456258Z"
        },
        "id": "j8jeyWoqnnY3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the aggregation function for test_person_1_feats_1\n",
        "def aggregate_test_person_1_feats_1(df):\n",
        "    agg_df = df.groupby(\"case_id\").agg(\n",
        "        mainoccupationinc_384A_max=pd.NamedAgg(column=\"mainoccupationinc_384A\", aggfunc=\"max\"),\n",
        "        mainoccupationinc_384A_any_selfemployed=pd.NamedAgg(column=\"incometype_1044T\", aggfunc=lambda x: (x == \"SELFEMPLOYED\").max())\n",
        "    ).reset_index()\n",
        "    return agg_df\n",
        "\n",
        "# Apply the aggregation function\n",
        "test_person_1_feats_1 = aggregate_test_person_1_feats_1(test_person_1)\n",
        "\n",
        "# Filter and select operations for test_person_1_feats_2\n",
        "test_person_1_feats_2 = test_person_1.loc[test_person_1[\"num_group1\"] == 0, [\"case_id\", \"housetype_905L\"]]\n",
        "test_person_1_feats_2.rename(columns={\"housetype_905L\": \"person_housetype\"}, inplace=True)\n",
        "\n",
        "# Define the aggregation function for test_credit_bureau_b_2_feats\n",
        "def aggregate_test_credit_bureau_b_2_feats(df):\n",
        "    agg_df = df.groupby(\"case_id\").agg(\n",
        "        pmts_pmtsoverdue_635A_max=pd.NamedAgg(column=\"pmts_pmtsoverdue_635A\", aggfunc=\"max\"),\n",
        "        pmts_dpdvalue_108P_over31=pd.NamedAgg(column=\"pmts_dpdvalue_108P\", aggfunc=lambda x: (x > 31).max())\n",
        "    ).reset_index()\n",
        "    return agg_df\n",
        "\n",
        "# Apply the aggregation function\n",
        "test_credit_bureau_b_2_feats = aggregate_test_credit_bureau_b_2_feats(test_credit_bureau_b_2)\n",
        "\n",
        "# Joining tables together\n",
        "data_submission = test_basetable.merge(\n",
        "    test_static[[\"case_id\"] + selected_static_cols], how=\"left\", on=\"case_id\"\n",
        ").merge(\n",
        "    test_static_cb[[\"case_id\"] + selected_static_cb_cols], how=\"left\", on=\"case_id\"\n",
        ").merge(\n",
        "    test_person_1_feats_1, how=\"left\", on=\"case_id\"\n",
        ").merge(\n",
        "    test_person_1_feats_2, how=\"left\", on=\"case_id\"\n",
        ").merge(\n",
        "    test_credit_bureau_b_2_feats, how=\"left\", on=\"case_id\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T21:25:18.477539Z",
          "iopub.status.busy": "2024-02-07T21:25:18.47705Z",
          "iopub.status.idle": "2024-02-07T21:25:28.525841Z",
          "shell.execute_reply": "2024-02-07T21:25:28.524407Z",
          "shell.execute_reply.started": "2024-02-07T21:25:18.477494Z"
        },
        "id": "n1A0qUWXnnY4",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['amtinstpaidbefduel24m_4187115A', 'annuity_780A', 'annuitynextmonth_57A', 'avginstallast24m_3658937A', 'avglnamtstart24m_4525187A', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'credamount_770A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'disbursedcredamount_1113A', 'downpmt_116A', 'inittransactionamount_650A', 'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M', 'lastapprcredamount_781A', 'lastcancelreason_561M', 'lastotherinc_902A', 'lastotherlnsexpense_631A', 'lastrejectcommoditycat_161M', 'lastrejectcommodtypec_5251769M', 'lastrejectcredamount_222A', 'lastrejectreason_759M', 'lastrejectreasonclient_4145040M', 'maininc_215A', 'maxannuity_159A', 'maxannuity_4075009A', 'maxdebt4_972A', 'maxinstallast24m_3658928A', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'maxpmtlast3m_4525190A', 'previouscontdistrict_112M', 'price_1097A', 'sumoutstandtotal_3546847A', 'sumoutstandtotalest_4493215A', 'totaldebt_9A', 'totalsettled_863A', 'totinstallast1m_4525188A', 'description_5085714M', 'education_1103M', 'education_88M', 'maritalst_385M', 'maritalst_893M', 'pmtaverage_3A', 'pmtaverage_4527227A', 'pmtaverage_4955615A', 'pmtssum_45A']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Shuffle the case_ids and split into train, validation, and test sets\n",
        "case_ids = data[\"case_id\"].unique()\n",
        "case_ids = pd.Series(case_ids).sample(frac=1, random_state=1).to_numpy()\n",
        "case_ids_train, case_ids_test = train_test_split(case_ids, train_size=0.6, random_state=1)\n",
        "case_ids_valid, case_ids_test = train_test_split(case_ids_test, train_size=0.5, random_state=1)\n",
        "\n",
        "# Extract columns for prediction\n",
        "cols_pred = [col for col in data.columns if col[-1].isupper() and col[:-1].islower()]\n",
        "\n",
        "print(cols_pred)\n",
        "\n",
        "# Function to filter data and convert to pandas DataFrame\n",
        "def from_polars_to_pandas(case_ids, data):\n",
        "    filtered_data = data[data[\"case_id\"].isin(case_ids)]\n",
        "    return (\n",
        "        filtered_data[[\"case_id\", \"WEEK_NUM\", \"target\"]],\n",
        "        filtered_data[cols_pred],\n",
        "        filtered_data[\"target\"]\n",
        "    )\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "base_train, X_train, y_train = from_polars_to_pandas(case_ids_train, data)\n",
        "base_valid, X_valid, y_valid = from_polars_to_pandas(case_ids_valid, data)\n",
        "base_test, X_test, y_test = from_polars_to_pandas(case_ids_test, data)\n",
        "\n",
        "# Convert string columns to category\n",
        "for df in [X_train, X_valid, X_test]:\n",
        "    df = convert_strings(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Putting the Dataframees into excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-02-07T21:25:28.529014Z",
          "iopub.status.busy": "2024-02-07T21:25:28.52813Z",
          "iopub.status.idle": "2024-02-07T21:25:28.534479Z",
          "shell.execute_reply": "2024-02-07T21:25:28.53334Z",
          "shell.execute_reply.started": "2024-02-07T21:25:28.528913Z"
        },
        "id": "HnYkfG5xnnY4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(f\"Train: {X_train.shape}\")\n",
        "print(f\"Valid: {X_valid.shape}\")\n",
        "print(f\"Test: {X_test.shape}\")\n",
        "\n",
        "# Exporting the nulls count to an Excel file\n",
        "with pd.ExcelWriter(dataPath + 'Data_After.xlsx') as writer:\n",
        "    # X_train.head(int(len(X_train) / 100)).to_excel(writer, sheet_name='Train', index=False)\n",
        "    # X_valid.to_excel(writer, sheet_name='Valid', index=False)\n",
        "    # X_test.to_excel(writer, sheet_name='Test', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lightgmb model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Maevex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 10 rounds\n",
            "[100]\tvalid_0's auc: 0.702002\n",
            "[200]\tvalid_0's auc: 0.715652\n",
            "[300]\tvalid_0's auc: 0.724347\n",
            "[400]\tvalid_0's auc: 0.729243\n"
          ]
        }
      ],
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
        "lgb_valid = lgb.Dataset(X_valid, label=y_valid, reference=lgb_train)\n",
        "\n",
        "params = {\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"objective\": \"binary\",\n",
        "    \"metric\": \"auc\",\n",
        "    \"max_depth\": 5,\n",
        "    \"num_leaves\": 31,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"feature_fraction\": 0.9,\n",
        "    \"bagging_fraction\": 0.8,\n",
        "    \"bagging_freq\": 5,\n",
        "    \"n_estimators\": 5000,\n",
        "    \"verbose\": -1,\n",
        "}\n",
        "\n",
        "\n",
        "gbm = lgb.train(\n",
        "    params,\n",
        "    lgb_train,\n",
        "    valid_sets=lgb_valid,\n",
        "    callbacks=[lgb.log_evaluation(100), lgb.early_stopping(10)]\n",
        "    \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extracting the evaluation results.\n",
        "results = gbm.evals_result_\n",
        "epochs = len(results['valid_0']['auc'])\n",
        "x_axis = range(0, epochs)\n",
        "\n",
        "# Plotting AUC vs Iteration\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(x_axis, results['valid_0']['auc'], label='AUC')\n",
        "plt.title('AUC vs Iterations')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('AUC')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Explaining the LightGBM Model\n",
        "### General Parameters\n",
        "- **`boosting_type`**: This specifies the type of boosting to use. Common options are:\n",
        "  - `\"gbdt\"`: Gradient Boosting Decision Tree (default).\n",
        "  - `\"dart\"`: Dropouts meet Multiple Additive Regression Trees.\n",
        "  - `\"goss\"`: Gradient-based One-Side Sampling.\n",
        "  - `\"rf\"`: Random Forest.\n",
        "\n",
        "### Objective and Metric Parameters\n",
        "- **`objective`**: This defines the learning task. In this case:\n",
        "  - `\"binary\"`: For binary classification, which aims to classify inputs into one of two categories.\n",
        "  \n",
        "- **`metric`**: This specifies the evaluation metric for validation data. In this case:\n",
        "  - `\"auc\"`: Area Under the Curve, commonly used for binary classification to evaluate the model's ability to distinguish between positive and negative classes.\n",
        "\n",
        "### Tree Parameters\n",
        "- **`max_depth`**: The maximum depth of a tree. Deeper trees can capture more complex patterns but may lead to overfitting. A `max_depth` of 5 is a moderate depth, balancing complexity and overfitting.\n",
        "\n",
        "- **`num_leaves`**: Maximum number of leaves in one tree. The number of leaves should be less than `2^(max_depth)`. More leaves can lead to a more complex model. Setting `num_leaves` to 31 controls the model's complexity.\n",
        "\n",
        "### Learning Control Parameters\n",
        "- **`learning_rate`**: This controls the step size at each iteration while moving toward a minimum of the loss function. A smaller learning rate requires more boosting iterations but can improve model accuracy. A `learning_rate` of 0.05 is considered moderate.\n",
        "\n",
        "### Feature and Data Sampling Parameters\n",
        "- **`feature_fraction`**: The fraction of features to be used for each boosting iteration. It helps to prevent overfitting by randomly selecting a subset of features. A `feature_fraction` of 0.9 means 90% of features are used in each iteration.\n",
        "\n",
        "- **`bagging_fraction`**: The fraction of data to be used for each boosting iteration (without resampling). This helps to prevent overfitting by introducing randomness. A `bagging_fraction` of 0.8 means 80% of the data is used in each iteration.\n",
        "\n",
        "- **`bagging_freq`**: Frequency of performing bagging, meaning the model will perform bagging once in every specified number of iterations. A `bagging_freq` of 5 means bagging is done every 5 iterations.\n",
        "\n",
        "### Training Control Parameters\n",
        "- **`n_estimators`**: Maximum number of boosting iterations. The model will train up to this number of iterations unless early stopping criteria are met. A high value like 1000 provides ample opportunity for the model to converge.\n",
        "\n",
        "- **`verbose`**: Controls the verbosity of the output. Setting `verbose` to -1 suppresses all messages, which can be useful to avoid clutter in the output.\n",
        "\n",
        "### Callbacks\n",
        "- **`lgb.log_evaluation(50)`**: This callback logs the evaluation results every 50 iterations. It helps in monitoring the training process.\n",
        "  \n",
        "- **`lgb.early_stopping(10)`**: This callback stops training if the model's performance on the validation set does not improve for 10 consecutive rounds. This prevents overfitting and saves computational resources by stopping training early when further training is unlikely to yield better results.\n",
        "\n",
        "### Summary\n",
        "These parameters collectively control various aspects of the LightGBM training process, such as the type of model to be used, the structure and complexity of the trees, the learning process, and measures to prevent overfitting. Proper tuning of these parameters is crucial to building a performant model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Home Credit 2024 Starter Notebook",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 7493015,
          "sourceId": 50160,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30635,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
